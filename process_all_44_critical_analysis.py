#!/usr/bin/env python3
"""Processamento dos 44 PDFs com anÃ¡lise crÃ­tica da fidelidade"""

import sys
import os
import time
import json
from pathlib import Path
from datetime import datetime

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from converter.pipeline import ConversionPipeline

def analyze_fidelity(pdf_path, md_content):
    """Analisa a fidelidade do conteÃºdo Markdown em relaÃ§Ã£o ao PDF original"""
    
    # MÃ©tricas de fidelidade
    fidelity_metrics = {
        'total_chars': len(md_content),
        'total_lines': len(md_content.split('\n')),
        'headers_count': len([line for line in md_content.split('\n') if line.startswith('#')]),
        'issues': [],
        'fidelity_score': 0
    }
    
    # Problemas crÃ­ticos que afetam fidelidade
    critical_issues = []
    
    # 1. Verificar duplicaÃ§Ãµes excessivas
    lines = md_content.split('\n')
    seen_lines = set()
    duplicates = 0
    for line in lines:
        line_stripped = line.strip()
        if line_stripped and line_stripped in seen_lines:
            duplicates += 1
        seen_lines.add(line_stripped)
    
    duplicate_percentage = (duplicates / len(lines)) * 100 if lines else 0
    if duplicate_percentage > 10:
        critical_issues.append(f"DuplicaÃ§Ãµes excessivas: {duplicate_percentage:.1f}%")
        fidelity_metrics['fidelity_score'] -= 20
    
    # 2. Verificar metadados incorretamente marcados como headers
    academic_metadata_headers = 0
    academic_patterns = [
        'Proceedings', 'Volume', 'Article', 'Print Reference', 'DOI:', 
        'Available at:', 'Follow this and additional works', 'CedarCommons repository',
        'Recommended Citation', 'Browse the contents', 'University', 'College',
        'Copyright', 'All Rights Reserved', 'Journal Policies', 'Editorial Board',
        'Assistant Editor:', 'Editor:', 'About the', 'Founded in', 'BSG membership',
        'Occas. Papers', 'www.', 'Email:', 'USA', 'Center for Origins',
        'Weatherford, TX', 'Dayton, TN', 'Tallahassee, FL', 'Joseph Francis',
        'Margaret Helder', 'Georg Huber', 'Richard Sternberg', 'Todd Charles Wood',
        'Kurt P. Wise', 'Roger Sanders', 'N. Doran'
    ]
    
    for line in lines:
        if line.startswith('#'):
            for pattern in academic_patterns:
                if pattern in line:
                    academic_metadata_headers += 1
                    break
    
    if academic_metadata_headers > 5:
        critical_issues.append(f"Muitos metadados como headers: {academic_metadata_headers}")
        fidelity_metrics['fidelity_score'] -= 15
    
    # 3. Verificar conteÃºdo muito pequeno (possÃ­vel falha na extraÃ§Ã£o)
    if len(md_content) < 5000:  # Menos de 5KB
        critical_issues.append(f"ConteÃºdo muito pequeno: {len(md_content)} chars")
        fidelity_metrics['fidelity_score'] -= 30
    
    # 4. Verificar fragmentaÃ§Ã£o excessiva (muitas linhas vazias)
    empty_lines = len([line for line in lines if not line.strip()])
    empty_percentage = (empty_lines / len(lines)) * 100 if lines else 0
    if empty_percentage > 30:
        critical_issues.append(f"Muitas linhas vazias: {empty_percentage:.1f}%")
        fidelity_metrics['fidelity_score'] -= 10
    
    # 5. Verificar estrutura hierÃ¡rquica
    headers = [line for line in lines if line.startswith('#')]
    if len(headers) == 0:
        critical_issues.append("Sem estrutura hierÃ¡rquica detectada")
        fidelity_metrics['fidelity_score'] -= 15
    elif len(headers) > len(lines) * 0.3:
        critical_issues.append("Estrutura hierÃ¡rquica excessiva")
        fidelity_metrics['fidelity_score'] -= 10
    
    # 6. Verificar palavras-chave acadÃªmicas importantes
    academic_keywords = ['abstract', 'introduction', 'method', 'results', 'discussion', 'conclusion', 'references']
    found_keywords = []
    for keyword in academic_keywords:
        if keyword in md_content.lower():
            found_keywords.append(keyword)
    
    if len(found_keywords) < 3:
        critical_issues.append(f"Poucas seÃ§Ãµes acadÃªmicas encontradas: {found_keywords}")
        fidelity_metrics['fidelity_score'] -= 10
    
    # Calcular score de fidelidade (base 100)
    fidelity_metrics['fidelity_score'] += 100
    fidelity_metrics['fidelity_score'] = max(0, fidelity_metrics['fidelity_score'])
    
    # Determinar status
    if fidelity_metrics['fidelity_score'] >= 95:
        status = "EXCELENTE"
    elif fidelity_metrics['fidelity_score'] >= 85:
        status = "BOM"
    elif fidelity_metrics['fidelity_score'] >= 70:
        status = "REGULAR"
    else:
        status = "POBRE"
    
    fidelity_metrics['status'] = status
    fidelity_metrics['critical_issues'] = critical_issues
    
    return fidelity_metrics

def process_all_pdfs_critical():
    """Processa todos os 44 PDFs com anÃ¡lise crÃ­tica"""
    
    # DiretÃ³rios
    pdf_dir = "/home/andrade/Documentos/Meus Ebooks/Genesis: The Correct Timeline/Referencias em PDF/"
    output_dir = "/home/andrade/Documentos/Meus Ebooks/Genesis: The Correct Timeline/Referencias em Markdown"
    
    # Listar todos os PDFs
    pdf_files = list(Path(pdf_dir).glob("*.pdf"))
    pdf_files.sort()
    
    print(f"ğŸ¯ PROCESSAMENTO CRÃTICO DOS {len(pdf_files)} PDFs")
    print(f"ğŸ“Š PadrÃ£o de qualidade: 95% de fidelidade")
    print("=" * 80)
    
    # Criar pipeline
    pipeline = ConversionPipeline(output_dir)
    
    # EstatÃ­sticas
    total_stats = {
        'processed': 0,
        'excellent': 0,
        'good': 0,
        'regular': 0,
        'poor': 0,
        'total_fidelity_score': 0,
        'failures': 0,
        'results': {}
    }
    
    start_time = time.time()
    
    for i, pdf_path in enumerate(pdf_files, 1):
        pdf_name = pdf_path.name
        print(f"\n[{i}/{len(pdf_files)}] ğŸ” ANÃLISE CRÃTICA: {pdf_name}")
        
        try:
            # Executar conversÃ£o
            result = pipeline.convert(str(pdf_path))
            
            if result and 'markdown_content' in result:
                markdown_content = result['markdown_content']
                
                # AnÃ¡lise crÃ­tica de fidelidade
                fidelity_metrics = analyze_fidelity(pdf_path, markdown_content)
                
                # Exibir resultados
                print(f"  ğŸ“Š Fidelidade: {fidelity_metrics['fidelity_score']:.1f}% - {fidelity_metrics['status']}")
                print(f"  ğŸ“ Tamanho: {fidelity_metrics['total_chars']:,} chars")
                print(f"  ğŸ“‹ Linhas: {fidelity_metrics['total_lines']}")
                print(f"  ğŸ·ï¸  Headers: {fidelity_metrics['headers_count']}")
                
                if fidelity_metrics['critical_issues']:
                    print(f"  âš ï¸  Problemas crÃ­ticos:")
                    for issue in fidelity_metrics['critical_issues']:
                        print(f"     - {issue}")
                else:
                    print(f"  âœ… Sem problemas crÃ­ticos detectados")
                
                # Salvar resultado
                output_file = os.path.join(output_dir, pdf_name.replace('.pdf', '.md'))
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(markdown_content)
                
                # Acumular estatÃ­sticas
                total_stats['processed'] += 1
                total_stats['total_fidelity_score'] += fidelity_metrics['fidelity_score']
                
                if fidelity_metrics['status'] == "EXCELENTE":
                    total_stats['excellent'] += 1
                elif fidelity_metrics['status'] == "BOM":
                    total_stats['good'] += 1
                elif fidelity_metrics['status'] == "REGULAR":
                    total_stats['regular'] += 1
                else:
                    total_stats['poor'] += 1
                
                total_stats['results'][pdf_name] = fidelity_metrics
                
                print(f"  ğŸ’¾ Arquivo salvo: {output_file}")
                
            else:
                print(f"  âŒ Falha na conversÃ£o")
                total_stats['failures'] += 1
                
        except Exception as e:
            print(f"  âŒ ERRO: {str(e)}")
            total_stats['failures'] += 1
    
    # EstatÃ­sticas finais
    elapsed_time = time.time() - start_time
    
    print(f"\n" + "=" * 80)
    print(f"ğŸ¯ RESULTADOS DA ANÃLISE CRÃTICA")
    print(f"=" * 80)
    print(f"ğŸ“ Total processado: {total_stats['processed']}/{len(pdf_files)}")
    print(f"â±ï¸  Tempo total: {elapsed_time:.1f} segundos")
    print(f"ğŸ“Š Tempo mÃ©dio: {elapsed_time/len(pdf_files):.1f} segundos/arquivo")
    
    if total_stats['processed'] > 0:
        avg_fidelity = total_stats['total_fidelity_score'] / total_stats['processed']
        print(f"ğŸ¯ Fidelidade mÃ©dia: {avg_fidelity:.1f}%")
    
    print(f"\nğŸ“ˆ DISTRIBUIÃ‡ÃƒO DE QUALIDADE:")
    print(f"  ğŸ‰ EXCELENTE (â‰¥95%): {total_stats['excellent']} arquivos")
    print(f"  ğŸ‘ BOM (85-94%): {total_stats['good']} arquivos")
    print(f"  âš ï¸  REGULAR (70-84%): {total_stats['regular']} arquivos")
    print(f"  âŒ POBRE (<70%): {total_stats['poor']} arquivos")
    print(f"  ğŸ’¥ FALHAS: {total_stats['failures']} arquivos")
    
    # AvaliaÃ§Ã£o geral
    print(f"\nğŸ¯ AVALIAÃ‡ÃƒO GERAL:")
    if avg_fidelity >= 95:
        print(f"  ğŸ‰ EXCELENTE! Sistema atingiu o padrÃ£o de 95% de fidelidade!")
    elif avg_fidelity >= 85:
        print(f"  ğŸ‘ BOM! Sistema prÃ³ximo do padrÃ£o desejado.")
    elif avg_fidelity >= 70:
        print(f"  âš ï¸  REGULAR. Melhorias necessÃ¡rias para atingir 95%.")
    else:
        print(f"  âŒ POBRE. RevisÃ£o completa necessÃ¡ria.")
    
    # Salvar relatÃ³rio detalhado
    report_file = f"relatorio_fidelidade_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(total_stats, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ RelatÃ³rio detalhado salvo: {report_file}")
    print(f"ğŸ“‚ Verifique os arquivos em: {output_dir}")

if __name__ == "__main__":
    process_all_pdfs_critical()
