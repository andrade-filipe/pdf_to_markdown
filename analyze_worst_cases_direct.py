#!/usr/bin/env python3
"""
An√°lise e convers√£o direta dos 5 piores casos
"""

import os
import json
import fitz
import re
from pathlib import Path
from collections import defaultdict
from converter.pipeline import ConversionPipeline

# Configura√ß√µes
PDF_DIR = "/home/andrade/Documentos/Meus Ebooks/Genesis: The Correct Timeline/Referencias em PDF"
MARKDOWN_DIR = "/home/andrade/Documentos/Meus Ebooks/Genesis: The Correct Timeline/Referencias em Markdown"

# Os 5 piores casos identificados
WORST_CASES = [
    "181014cronologia_ap.pdf",  # Score: 3/10 - Preserva√ß√£o: 305.8%
    "Shoreline Transgressive Terraces.pdf",  # Score: 4/10 - Preserva√ß√£o: 101.1%
    "Biostratigraphic Continuity and Earth History.pdf",  # Score: 4/10 - Preserva√ß√£o: 106.2%
    "coming-to-grips-with-genesis-ch6.pdf",  # Score: 4/10 - Preserva√ß√£o: 96.0%
    "Dark Matter and Dark Energy.pdf"  # Score: 4/10 - Preserva√ß√£o: 100.0%
]

def analyze_pdf_structure_detailed(pdf_path):
    """An√°lise detalhada da estrutura do PDF"""
    doc = fitz.open(pdf_path)
    
    analysis = {
        'total_pages': len(doc),
        'page_analysis': [],
        'text_blocks': [],
        'font_analysis': defaultdict(int),
        'line_analysis': {'total_lines': 0, 'empty_lines': 0, 'avg_line_length': 0},
        'total_chars': 0,
        'total_words': 0
    }
    
    total_lines = 0
    empty_lines = 0
    line_lengths = []
    all_text = ""
    
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        blocks = page.get_text("dict")
        
        page_analysis = {
            'page': page_num + 1,
            'blocks': len(blocks['blocks']),
            'text_blocks': 0,
            'image_blocks': 0,
            'text_content': '',
            'lines': []
        }
        
        for block in blocks['blocks']:
            if 'lines' in block:  # Text block
                page_analysis['text_blocks'] += 1
                
                for line in block['lines']:
                    line_text = ''
                    for span in line['spans']:
                        text = span['text'].strip()
                        if text:
                            line_text += text + ' '
                            analysis['font_analysis'][span['font']] += 1
                    
                    line_text = line_text.strip()
                    page_analysis['lines'].append({
                        'text': line_text,
                        'length': len(line_text),
                        'is_empty': len(line_text) == 0
                    })
                    
                    if line_text:
                        page_analysis['text_content'] += line_text + '\n'
                        all_text += line_text + ' '
                        total_lines += 1
                        line_lengths.append(len(line_text))
                    else:
                        empty_lines += 1
                        
            elif 'width' in block and 'height' in block:  # Image block
                page_analysis['image_blocks'] += 1
        
        analysis['page_analysis'].append(page_analysis)
    
    # Calcular estat√≠sticas de texto
    analysis['total_chars'] = len(all_text)
    analysis['total_words'] = len(all_text.split())
    
    analysis['line_analysis'] = {
        'total_lines': total_lines,
        'empty_lines': empty_lines,
        'avg_line_length': sum(line_lengths) / len(line_lengths) if line_lengths else 0,
        'line_density': total_lines / (total_lines + empty_lines) if (total_lines + empty_lines) > 0 else 0
    }
    
    doc.close()
    return analysis

def analyze_markdown_quality_detailed(md_path):
    """An√°lise detalhada da qualidade do Markdown"""
    try:
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except FileNotFoundError:
        return {'error': 'Arquivo n√£o encontrado'}
    
    lines = content.split('\n')
    non_empty_lines = [line for line in lines if line.strip()]
    
    analysis = {
        'total_lines': len(lines),
        'non_empty_lines': len(non_empty_lines),
        'empty_lines': len(lines) - len(non_empty_lines),
        'content_density': len(non_empty_lines) / len(lines) if lines else 0,
        'avg_line_length': sum(len(line) for line in non_empty_lines) / len(non_empty_lines) if non_empty_lines else 0,
        'total_chars': len(content),
        'total_words': len(content.split()),
        
        # An√°lise de estrutura
        'headers': len(re.findall(r'^#{1,6}\s', content, re.MULTILINE)),
        'paragraphs': len(re.findall(r'\n\n', content)),
        'lists': len(re.findall(r'^\s*[-*+]\s', content, re.MULTILINE)),
        
        # Problemas detectados
        'problems': []
    }
    
    # Detectar problemas espec√≠ficos
    if analysis['content_density'] < 0.3:
        analysis['problems'].append(f"Baixa densidade de conte√∫do: {analysis['content_density']:.2%}")
    
    if analysis['content_density'] > 0.9:
        analysis['problems'].append(f"Densidade muito alta (poss√≠vel problema): {analysis['content_density']:.2%}")
    
    # Verificar repeti√ß√µes excessivas
    lines_text = ' '.join(non_empty_lines)
    words = lines_text.split()
    if len(words) > 100:  # S√≥ analisar se houver conte√∫do suficiente
        word_freq = defaultdict(int)
        for word in words:
            word_freq[word.lower()] += 1
        
        repeated_words = [(word, count) for word, count in word_freq.items() if count > 20]
        if repeated_words:
            analysis['problems'].append(f"Palavras muito repetidas: {len(repeated_words)} (m√°x: {max(count for _, count in repeated_words)})")
    
    # Verificar quebras de linha excessivas
    consecutive_empty = 0
    max_empty = 0
    for line in lines:
        if not line.strip():
            consecutive_empty += 1
            max_empty = max(max_empty, consecutive_empty)
        else:
            consecutive_empty = 0
    
    if max_empty > 3:
        analysis['problems'].append(f"Muitas linhas vazias consecutivas: {max_empty}")
    
    # Verificar preserva√ß√£o excessiva (>100% pode indicar duplica√ß√£o)
    if 'total_chars' in analysis and analysis['total_chars'] > 0:
        char_density = len(lines_text) / analysis['total_chars']
        if char_density > 1.5:
            analysis['problems'].append(f"Poss√≠vel duplica√ß√£o de conte√∫do: densidade de caracteres {char_density:.2f}")
    
    return analysis

def main():
    print("üîç AN√ÅLISE E CONVERS√ÉO DIRETA DOS 5 PIORES CASOS")
    print("=" * 60)
    
    # Criar pipeline
    pipeline = ConversionPipeline(str(Path(MARKDOWN_DIR)))
    
    results = {}
    
    for pdf_name in WORST_CASES:
        print(f"\nüìÑ ANALISANDO: {pdf_name}")
        print("-" * 40)
        
        pdf_path = Path(PDF_DIR) / pdf_name
        md_path = Path(MARKDOWN_DIR) / f"{pdf_name.replace('.pdf', '.md')}"
        
        if not pdf_path.exists():
            print(f"‚ùå PDF n√£o encontrado: {pdf_path}")
            continue
        
        # 1. An√°lise do PDF original
        print("üìä Analisando estrutura do PDF...")
        pdf_analysis = analyze_pdf_structure_detailed(pdf_path)
        
        print(f"   üìÑ P√°ginas: {pdf_analysis['total_pages']}")
        print(f"   üìù Linhas totais: {pdf_analysis['line_analysis']['total_lines']}")
        print(f"   üìä Linhas vazias: {pdf_analysis['line_analysis']['empty_lines']}")
        print(f"   üìà Densidade: {pdf_analysis['line_analysis']['line_density']:.3f}")
        print(f"   üìè Comprimento m√©dio: {pdf_analysis['line_analysis']['avg_line_length']:.1f}")
        print(f"   üî¢ Caracteres: {pdf_analysis['total_chars']:,}")
        print(f"   üìö Palavras: {pdf_analysis['total_words']:,}")
        
        # 2. Convers√£o para Markdown (sobrescrevendo)
        print("\nüîÑ Convertendo para Markdown...")
        try:
            output_filename = f"{pdf_name.replace('.pdf', '.md')}"
            output_path = pipeline.convert(str(pdf_path), output_filename)
            print(f"   ‚úÖ Convers√£o conclu√≠da: {output_path}")
            
            # 3. An√°lise do Markdown convertido
            print("\nüìù Analisando Markdown convertido...")
            md_analysis = analyze_markdown_quality_detailed(output_path)
            
            if 'error' not in md_analysis:
                print(f"   üìä Linhas totais: {md_analysis['total_lines']}")
                print(f"   üìù Linhas com conte√∫do: {md_analysis['non_empty_lines']}")
                print(f"   üìà Densidade: {md_analysis['content_density']:.3f}")
                print(f"   üìè Comprimento m√©dio: {md_analysis['avg_line_length']:.1f}")
                print(f"   üî¢ Caracteres: {md_analysis['total_chars']:,}")
                print(f"   üìö Palavras: {md_analysis['total_words']:,}")
                print(f"   üè∑Ô∏è Headers: {md_analysis['headers']}")
                print(f"   üìã Listas: {md_analysis['lists']}")
                
                # Calcular preserva√ß√£o
                char_preservation = md_analysis['total_chars'] / pdf_analysis['total_chars'] * 100
                word_preservation = md_analysis['total_words'] / pdf_analysis['total_words'] * 100
                
                print(f"   üìà Preserva√ß√£o de caracteres: {char_preservation:.1f}%")
                print(f"   üìö Preserva√ß√£o de palavras: {word_preservation:.1f}%")
                
                if md_analysis['problems']:
                    print(f"   ‚ö†Ô∏è Problemas detectados:")
                    for problem in md_analysis['problems']:
                        print(f"      - {problem}")
                
                # 4. An√°lise do Markdown original (se existir)
                if md_path.exists():
                    print("\nüìÑ Analisando Markdown original...")
                    original_analysis = analyze_markdown_quality_detailed(md_path)
                    
                    if 'error' not in original_analysis:
                        print(f"   üìà Densidade original: {original_analysis['content_density']:.3f}")
                        print(f"   üìä Linhas originais: {original_analysis['non_empty_lines']}")
                        
                        improvement = md_analysis['content_density'] - original_analysis['content_density']
                        print(f"   üîÑ Mudan√ßa na densidade: {improvement:+.3f}")
                        
                        if improvement > 0.1:
                            print(f"   üöÄ MELHORIA SIGNIFICATIVA DETECTADA!")
                
                results[pdf_name] = {
                    'pdf': pdf_analysis,
                    'markdown': md_analysis,
                    'char_preservation': char_preservation,
                    'word_preservation': word_preservation
                }
            else:
                print(f"   ‚ùå Erro: {md_analysis['error']}")
                
        except Exception as e:
            print(f"   ‚ùå Erro na convers√£o: {e}")
    
    # 5. An√°lise geral dos problemas
    print("\nüéØ AN√ÅLISE GERAL DOS PROBLEMAS")
    print("=" * 60)
    
    if results:
        common_issues = defaultdict(int)
        preservation_stats = []
        
        for pdf_name, result in results.items():
            if 'markdown' in result:
                for problem in result['markdown']['problems']:
                    common_issues[problem] += 1
                
                preservation_stats.append({
                    'file': pdf_name,
                    'char_preservation': result['char_preservation'],
                    'word_preservation': result['word_preservation'],
                    'density': result['markdown']['content_density']
                })
        
        print("üìä Problemas mais comuns:")
        for problem, count in sorted(common_issues.items(), key=lambda x: x[1], reverse=True):
            print(f"   - {problem}: {count} arquivos")
        
        print("\nüìà Estat√≠sticas de preserva√ß√£o:")
        for stat in preservation_stats:
            status = "‚ö†Ô∏è ALTO" if stat['char_preservation'] > 120 else "‚úÖ NORMAL" if stat['char_preservation'] >= 80 else "‚ùå BAIXO"
            print(f"   {stat['file']}: {stat['char_preservation']:.1f}% chars, {stat['word_preservation']:.1f}% palavras, densidade {stat['density']:.3f} {status}")
    
    # Salvar an√°lise detalhada
    output_file = "worst_cases_direct_analysis.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nüíæ An√°lise salva em: {output_file}")
    print("\nüìÅ Arquivos convertidos est√£o em:")
    print(f"   {MARKDOWN_DIR}")
    
    return results

if __name__ == "__main__":
    main()
